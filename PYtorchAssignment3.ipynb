{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0bTYUoG0vBN"
   },
   "source": [
    "**Name : Dikshant Oswal**\n",
    "\n",
    "*   Roll No : 37\n",
    "*   ERP : 1132200219\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9FVUDelDx3nT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "piBq1w86yDn-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GCGY7LR4yQLE"
   },
   "outputs": [],
   "source": [
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zL6JV6wzyTMg"
   },
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q1031wrHyVkq"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsjhIJgpyZAq",
    "outputId": "09ace0e6-9b25-4ea4-b65f-31b8e91172a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(138.4185, grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 138.41851806640625\n",
      "tensor(11.5467, grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 11.546738624572754\n",
      "tensor(1.1953, grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 1.195343255996704\n",
      "tensor(0.3482, grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.34818166494369507\n",
      "tensor(0.2763, grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.2762818932533264\n",
      "tensor(0.2676, grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.2676491141319275\n",
      "tensor(0.2642, grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.2642081379890442\n",
      "tensor(0.2612, grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.2612209916114807\n",
      "tensor(0.2583, grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.2583005428314209\n",
      "tensor(0.2554, grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.2554160952568054\n",
      "tensor(0.2526, grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.2525639832019806\n",
      "tensor(0.2497, grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.24974364042282104\n",
      "tensor(0.2470, grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.24695466458797455\n",
      "tensor(0.2442, grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.24419693648815155\n",
      "tensor(0.2415, grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.2414700835943222\n",
      "tensor(0.2388, grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.2387734055519104\n",
      "tensor(0.2361, grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.23610743880271912\n",
      "tensor(0.2335, grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.23347067832946777\n",
      "tensor(0.2309, grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.23086342215538025\n",
      "tensor(0.2283, grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.22828556597232819\n",
      "tensor(0.2257, grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.22573629021644592\n",
      "tensor(0.2232, grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.22321563959121704\n",
      "tensor(0.2207, grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.2207227647304535\n",
      "tensor(0.2183, grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.2182580828666687\n",
      "tensor(0.2158, grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.21582083404064178\n",
      "tensor(0.2134, grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.2134108692407608\n",
      "tensor(0.2110, grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.21102763712406158\n",
      "tensor(0.2087, grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.2086709588766098\n",
      "tensor(0.2063, grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.2063409686088562\n",
      "tensor(0.2040, grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.20403671264648438\n",
      "tensor(0.2018, grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.20175832509994507\n",
      "tensor(0.1995, grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.1995055228471756\n",
      "tensor(0.1973, grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.1972772628068924\n",
      "tensor(0.1951, grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.1950744241476059\n",
      "tensor(0.1929, grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.19289611279964447\n",
      "tensor(0.1907, grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.19074216485023499\n",
      "tensor(0.1886, grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.1886121779680252\n",
      "tensor(0.1865, grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.18650588393211365\n",
      "tensor(0.1844, grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.18442335724830627\n",
      "tensor(0.1824, grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.18236388266086578\n",
      "tensor(0.1803, grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.1803274154663086\n",
      "tensor(0.1783, grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.17831379175186157\n",
      "tensor(0.1763, grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.17632266879081726\n",
      "tensor(0.1744, grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.17435374855995178\n",
      "tensor(0.1724, grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.17240674793720245\n",
      "tensor(0.1705, grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.1704813688993454\n",
      "tensor(0.1686, grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.16857758164405823\n",
      "tensor(0.1667, grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.16669513285160065\n",
      "tensor(0.1648, grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.16483359038829803\n",
      "tensor(0.1630, grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.16299283504486084\n",
      "tensor(0.1612, grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.1611727774143219\n",
      "tensor(0.1594, grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.15937301516532898\n",
      "tensor(0.1576, grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.157593235373497\n",
      "tensor(0.1558, grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.1558334231376648\n",
      "tensor(0.1541, grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.1540934443473816\n",
      "tensor(0.1524, grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.15237264335155487\n",
      "tensor(0.1507, grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.15067097544670105\n",
      "tensor(0.1490, grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.14898858964443207\n",
      "tensor(0.1473, grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.14732490479946136\n",
      "tensor(0.1457, grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.14567960798740387\n",
      "tensor(0.1441, grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.1440528929233551\n",
      "tensor(0.1424, grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.1424444168806076\n",
      "tensor(0.1409, grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.1408536285161972\n",
      "tensor(0.1393, grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.1392807960510254\n",
      "tensor(0.1377, grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.13772548735141754\n",
      "tensor(0.1362, grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.13618741929531097\n",
      "tensor(0.1347, grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.13466675579547882\n",
      "tensor(0.1332, grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.13316287100315094\n",
      "tensor(0.1317, grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.1316758543252945\n",
      "tensor(0.1302, grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.13020548224449158\n",
      "tensor(0.1288, grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.1287514716386795\n",
      "tensor(0.1273, grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.12731385231018066\n",
      "tensor(0.1259, grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.12589210271835327\n",
      "tensor(0.1245, grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.12448631972074509\n",
      "tensor(0.1231, grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.12309610098600388\n",
      "tensor(0.1217, grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.12172160297632217\n",
      "tensor(0.1204, grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.12036242336034775\n",
      "tensor(0.1190, grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.11901824176311493\n",
      "tensor(0.1177, grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.11768906563520432\n",
      "tensor(0.1164, grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.11637496203184128\n",
      "tensor(0.1151, grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.11507551372051239\n",
      "tensor(0.1138, grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.11379020661115646\n",
      "tensor(0.1125, grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.1125197634100914\n",
      "tensor(0.1113, grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.11126318573951721\n",
      "tensor(0.1100, grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.11002083122730255\n",
      "tensor(0.1088, grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.10879214107990265\n",
      "tensor(0.1076, grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.10757724940776825\n",
      "tensor(0.1064, grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.10637609660625458\n",
      "tensor(0.1052, grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.10518825054168701\n",
      "tensor(0.1040, grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.10401363670825958\n",
      "tensor(0.1029, grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.10285201668739319\n",
      "tensor(0.1017, grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.10170351713895798\n",
      "tensor(0.1006, grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.10056774318218231\n",
      "tensor(0.0994, grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.09944486618041992\n",
      "tensor(0.0983, grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.0983341857790947\n",
      "tensor(0.0972, grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.09723604470491409\n",
      "tensor(0.0962, grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.09615042805671692\n",
      "tensor(0.0951, grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.09507649391889572\n",
      "tensor(0.0940, grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.09401502460241318\n",
      "tensor(0.0930, grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.09296517819166183\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "3lJ7CcnsydHw",
    "outputId": "1efac98c-fd10-47c0-83b1-e98806c6a955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.432821 ]\n",
      " [ 2.5145   ]\n",
      " [ 4.5961785]\n",
      " [ 6.6778574]\n",
      " [ 8.759537 ]\n",
      " [10.841215 ]\n",
      " [12.9228945]\n",
      " [15.004574 ]\n",
      " [17.086252 ]\n",
      " [19.16793  ]\n",
      " [21.249609 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
